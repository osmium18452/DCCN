\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{xcolor}
\usepackage{subfiles}
\usepackage{subcaption}

\title{Spectral-Spatial Hyper-spectral Image Classification Using Dual-Channel Capsule Networks}
\author{liu wenbo}
\date{March 2020}

\newcommand\crule[3][black]{\textcolor{#1}{\rule{#2}{#3}}}
%color map definition
%pavia c
\definecolor{paviaU0}{RGB}{0, 0, 0}
\definecolor{paviaU1}{RGB}{219, 94, 86}
\definecolor{paviaU2}{RGB}{219, 183, 86}
\definecolor{paviaU3}{RGB}{167, 219, 86}
\definecolor{paviaU4}{RGB}{86, 219, 94}
\definecolor{paviaU5}{RGB}{86, 219, 183}
\definecolor{paviaU6}{RGB}{86, 167, 219}
\definecolor{paviaU7}{RGB}{94, 86, 219}
\definecolor{paviaU8}{RGB}{183, 86, 219}
\definecolor{paviaU9}{RGB}{219, 86, 167}
%pavia c
\definecolor{pavia0}{RGB}{0, 0, 0}
\definecolor{pavia1}{RGB}{219, 94, 86}
\definecolor{pavia2}{RGB}{219, 183, 86}
\definecolor{pavia3}{RGB}{167, 219, 86}
\definecolor{pavia4}{RGB}{86, 219, 94}
\definecolor{pavia5}{RGB}{86, 219, 183}
\definecolor{pavia6}{RGB}{86, 167, 219}
\definecolor{pavia7}{RGB}{94, 86, 219}
\definecolor{pavia8}{RGB}{183, 86, 219}
\definecolor{pavia9}{RGB}{219, 86, 167}
%salinas
\definecolor{salinas_corrected0}{RGB}{0, 0, 0}
\definecolor{salinas_corrected1}{RGB}{219, 94, 86}
\definecolor{salinas_corrected2}{RGB}{219, 144, 86}
\definecolor{salinas_corrected3}{RGB}{219, 194, 86}
\definecolor{salinas_corrected4}{RGB}{194, 219, 86}
\definecolor{salinas_corrected5}{RGB}{145, 219, 86}
\definecolor{salinas_corrected6}{RGB}{95, 219, 86}
\definecolor{salinas_corrected7}{RGB}{86, 219, 127}
\definecolor{salinas_corrected8}{RGB}{86, 219, 177}
\definecolor{salinas_corrected9}{RGB}{86, 211, 219}
\definecolor{salinas_corrected10}{RGB}{86, 161, 219}
\definecolor{salinas_corrected11}{RGB}{86, 111, 219}
\definecolor{salinas_corrected12}{RGB}{111, 86, 219}
\definecolor{salinas_corrected13}{RGB}{160, 86, 219}
\definecolor{salinas_corrected14}{RGB}{210, 86, 219}
\definecolor{salinas_corrected15}{RGB}{219, 86, 178}
\definecolor{salinas_corrected16}{RGB}{219, 86, 128}
%salinas a
\definecolor{salinasA_corrected0}{RGB}{0, 0, 0}
\definecolor{salinasA_corrected1}{RGB}{219, 94, 86}
\definecolor{salinasA_corrected2}{RGB}{211, 219, 86}
\definecolor{salinasA_corrected3}{RGB}{86, 219, 94}
\definecolor{salinasA_corrected4}{RGB}{86, 211, 219}
\definecolor{salinasA_corrected5}{RGB}{94, 86, 219}
\definecolor{salinasA_corrected6}{RGB}{219, 86, 211}


\begin{document}
	\bibliographystyle{IEEEtran}

	\maketitle


	\section{ABSTRACT}\label{sec:abstract}
	Deep learning methods have shown their marvel performance on hyper-spectral image (HSI) classification tasks.
	In particular,algorithms based on Convolution Neural Network (CNN) outperforms most of the conventional machine-
	learning based algorithms.
	Recently, a newly proposed neural network called Capsule Network (CapsNet) showed its potential to replace the CNNs
	in various classification tasks with its amazing performance.
	In this paper, we proposed a new network architecture based on CapsNet for HSI classification tasks, called
	Dual-channel Capsule Network (DCCN).
	Our DCCN model extracts features from spectral and spatial domain respectively with two separate convolution
	channels and then concatenates and feds the features into the capsule layer to classify each of the HSI pixels.
	The model is trained and tested on \textbf{(2/3/4/5?)} real HSI dataset \textbf{(Pavia University, Pavia City,
	Salinas Valley, Salinas A and KSC)} and achieved high accuracy.
	We also compared our network with some of the state-of-art models and found our model outperformed these models.
	Among these state-of-art models we also implemented the Capsule Network that has the same architecture as our
	network except the spectral convolution channel, and we found the DCCN model was superior to it as well.

	\noindent \textbf{Key Words:} Dual-Channel Capsule Network, Hyper-Spectral Image, Image Classification.


	\section{INTRODUCTION}\label{sec:introduction}
	With technology developing, high resolution hyper-spectral images (HSI) became readily available.
	Containing hundreds of spectral bands, HSI becomes a ideal method to further explore the earth's
	surface\cite{du2013foreword,bioucas2013hyperspectral,shippert2003introduction}.
	With a wealth of information, HSI has been applied in various fields, e.g.,
	land cover classification\cite{yan2015urban},
	object detection\cite{eslami2015developing},
	forest inventory\cite{matsuki2015hyperspectral}
	and water source management\cite{govender2007review}.
	Therefore, HSI classification has become a quite significant problem to solve.
	It aims at assigning a specific label to each pixel according to its spectral-spatial information\cite{wang2018scene}.

	The remainder of the paper is organized as follows.
	In section~\ref{sec:related-works}, we introduce some related work of our paper.
	In section~\ref{sec:proposed-method}, we describe the structure of our proposed network, how we pre-process the data
	and how to train the network.
	In section~\ref{sec:experiment}, we present and analyze the result of out experiment.
	In section~\ref{sec:discussion}, we further explore the potential of DCCN\@.
	Finally, in section~\ref{sec:conclusion}, we conclude this paper with some remarks and hints at plausible future
	research lines.


	\section{RELATED WORK}\label{sec:related-works}

	\subsection{Capsule Network}\label{subsec:capsule-network}

	\subsection{Margin Loss}\label{subsec:margin-loss}


	\section{PROPOSED METHOD}\label{sec:proposed-method}

	In this section, we introduce our proposed algorithm.
	We first described how we pre-process the HSI data, then we introduced the network structure and finally we
	introduced the hyper parameters we adopted to train the network.

	\subsection{Data Pre-Process}\label{subsec:data-pre-process}
	The original HSI dataset is usually a chunk of 3D tensor that is too large to feed into the neural network, so we
	have to do some process before input it into the DCCN\@.
	On one hand, we extracted the pixel with its neighbour pixels' spectrum and grouped them as a spectral cube, namely
	a patch.
	On the other, we extracted the single pixel's spectrum as a 1D vector.
	The pixel's spectrum vector will be combined with its corresponding patch to feed into the DCCN model simultaneously.
	The patch an be assigned with different size.
	In our experiment, we set it as 7×7×$n_{bands}$.

	\subsection{Network Architecture}\label{subsec:network-architecture}
	\begin{figure}[!ht]
		\centering
		\includegraphics[width=0.4\textwidth]{pic/lossAndAcc.eps}
		\caption{Proposed network architecture.}
		\label{netArct}
	\end{figure}
	Our proposed network architecture is showed in~\ref{netArct}.
	As is shown, our Dual-Channel Capsule Network contains two convolution channel, i.e., a 2D convolution channel to
	process the patch and a 1D convolution channel to process the spectrum, and a capsule channel.
	As we can see, the Dual-Channel Capsule Network we adopted is a very shallow network, containing only six layers,
	that is, a convolution layer append with a activation layer and a primary capsule layer in both of the spectral
	channel and the spatial channel, two capsule layer in the fully connected capsule channel and a output layer to
	process the output from the latter capsule layer.

	To be specific, the detailed parameter of the DCCN deployed in our experiment are as follows.
	For the spatial channel, the first layer is a 2D convolution layer, including 50 filters with the kernel size of
	3×3 and stride of 1.
	The following activation layer uses ReLU function as activation function.
	The primary capsule layer uses 64 filters with kernel size of 3×3 and stride of 1.
	Its output capsules' dimension is 8.
	For the spectral channel, the first layer is a 1D convolution layer,including 30 filters with kernel size of 32 and
	stride of 8.
	The following activation layer used ReLU activation function as well.
	The primary capsule layer uses 64 filters with kernel size of 3 and stride of 1.
	Its output capsules' dimension is 8 so that it can be concatenated with the spatial channel's output capsules and
	then fed into the fully connected capsule layer.
	For the first capsule layer, it has \textbf{100 or so?} capsules, which each contains 8 neurons inside.
	The second capsule layer has $n_{classes}$ capsules with 16 neurons inside, where $n_{classes}$ denotes how many
	different classes of pixels inside the HSI dataset.
	So far, the main part of the DCCN model has finished.
	But as is mentioned before, the method we proposed is end-to-end, so we added a output layer to process the output
	of the network.
	The output layer will squash the 2D tensor ,made up by capsules, into a 1D vector (Let's call the 1D vector
	probability vector ,prob-vector in short, since each of its elements represents the probability of a pixel should
	be assigned to the according class.) by replacing each of the capsule with its module and perform argument-max
	($argmax$) function to gain the pixel's label.

	And here's how the data flows in the DCCN.
	First, the sliced patch is fed into the 2D convolution channel to extract features spatially, and the spectrum
	of the patch's center pixel is fed into the 1D convolution channel to extract features in spectral domain in
	parallel.
	Then, the extracted features, both spectral and spatial, are concatenated and fed into the capsule channel to output
	the vectors (that is, the capsule) whose module represents the probability of a pixel being assigned a specific
	label.
	And finally, the output layer will squash the outputted 2D tensor and assign a label to the pixel by performing
	$argmax$ function to the prob-vector.

	\subsection{Train the Network}\label{subsec:train-the-network}
	At first, we randomly shuffled the training set and divided it into several batches.
	These batches ,instead of single pixel, would be feed into the model for training to accelerate the training process.
	In our experiment, considering the training speed and final accuracy, we set the batch size to 50.
	As for the loss function, we adopted the margin loss mentioned in section~\ref{sec:related-works}, instead of the
	usually used cross-entropy loss function.
	The network is trained using a first-order gradient-based optimization of stochastic objection, referred to as the
	Adam algorithm.
	Therefore, our training goal is to minimize the margin loss using the Adam optimizer, and we set the learning rate
	to 0.0001.
	When all batches have been fed into the network and finished training, we say the training process finished an epoch.
	In our experiment, we conducted 100 epochs for each of the dataset.



	\section{EXPERIMENT}\label{sec:experiment}
	In this section, we showed and analyzed the training result.
	We compared our model with six neural network based algorithms i.e.,
	1D CNN\cite{hu2015deep},
	2D CNN\cite{sharma2016hyperspectral},
	3D CNN\cite{hamida20183},
	3D FCN\cite{lee2016contextual},
	CNN-MRF\cite{cao2018hyperspectral},
	the CapsNet that is identical to our DCCN model except the spectral channel, and the machine learning algorithm
	SVM\@.
	The competing neural network based algorithms are trained using the best network structure and hyper-parameters
	proposed by the authors in their papers, and the SVM algorithm was trained using 10\% of the total labeled pixels
	as the DCCN model.
	The accuracy metrics we used include overall accuracy(OA), average accuracy(AA) and Kappa Coefficient($\kappa$).

	\subsection{Dataset Introduction}\label{subsec:dataset-introduction}
	In our experiment, we selected 4 real HSI dataset to train and test our DCCN model separately, that is, the
	Pavia University(PU) dataset, the Pavia Center(PC) dataset, the Salinas Valley(SV) dataset and the Salinas A(SA)
	dataset, which is a small subset of the Salinas Valley dataset.
	The first three of them are relatively large dataset and the last one is relatively small.
	We use these 4 dataset to test the robustness of out model when performing on different scale dataset.
	Here follows the brief introduction of the four dataset(see table\ref{psgt}).
	\begin{enumerate}[1)]
		\item \textit{Pavia Center and University} These are two scenes acquired by the ROSIS sensor during a flight
		campaign over Pavia, northern Italy.
		The number of spectral bands is 102 for Pavia Centre and 103 for Pavia University.
		Pavia Centre is a 1096*1096 pixels image, and Pavia University is 610*610 pixels, but some of the samples in
		both images contain no information and have to be discarded before the analysis, so the final picture size is
		1096*715 and 610*340.
		The geometric resolution is 1.3 meters.
		Both image has 9 different ground truth classes each.
		\item \textit{Salinas Scene} The Salinas scene was collected by the 224-band AVIRIS sensor over Salinas Valley,
		California, and is characterized by high spatial resolution (3.7-meter pixels).
		The Salinas Valley dataset is a 512*217 pixels image, and the Salinas A is a subset of the Salinas Valley
		containing 86*83 pixels.
		The Salinas scene dataset has been discarded 20 water absorption bands from the original image, so there's 204
		bands left in both of the tow dataset.
		The Salinas Valley dataset has 16 different classes but the Salinas A only has 6 different classes.
	\end{enumerate}
	\begin{table}[h]
		\centering
		\tiny
		\resizebox{\textwidth}{!}{
			\begin{tabular}{|c|c|c|c|}
				
				\hline
				\textbf{PAVIA CENTER} & \textbf{PAVIA UNIVERSITY}&\textbf{SALINAS VALLEY}  & \textbf{SALINAS A}\\
				\hline
				\includegraphics[width=0.4\textwidth]{pic/pavia_gt.eps}&
				\includegraphics[width=0.4\textwidth]{pic/paviaU_gt.eps}&
				\includegraphics[width=0.4\textwidth]{pic/salinas_gt.eps}&
				\includegraphics[width=0.4\textwidth]{pic/salinasA_gt.eps}\\
				\hline
				\begin{tabular}[t]{ccc}
					
					\textit{color}&\textit{land-cover type}&\textit{number}\\
					\hline
					\crule[pavia1]{0.02\textwidth}{0.02\textwidth} &Water &824    \\
					\crule[pavia2]{0.02\textwidth}{0.02\textwidth} &Trees &820    \\
					\crule[pavia3]{0.02\textwidth}{0.02\textwidth} &Asphalt &816    \\
					\crule[pavia4]{0.02\textwidth}{0.02\textwidth} &Self-Blocking Bricks &808    \\
					\crule[pavia5]{0.02\textwidth}{0.02\textwidth} &Bitumen &808    \\
					\crule[pavia6]{0.02\textwidth}{0.02\textwidth} &Tiles &1260   \\
					\crule[pavia7]{0.02\textwidth}{0.02\textwidth} &Shadows &476    \\
					\crule[pavia8]{0.02\textwidth}{0.02\textwidth} &Meadows &824    \\
					\crule[pavia9]{0.02\textwidth}{0.02\textwidth} &Bare Soil &820    \\
				\end{tabular}&
				\begin{tabular}[t]{ccc}
					
					\textit{color}&\textit{land-cover type}&\textit{number}\\
					\hline
					\crule[paviaU1]{0.02\textwidth}{0.02\textwidth} &Asphalt &6631   \\
					\crule[paviaU2]{0.02\textwidth}{0.02\textwidth} &Meadows &18649  \\
					\crule[paviaU3]{0.02\textwidth}{0.02\textwidth} &Gravel &2099   \\
					\crule[paviaU4]{0.02\textwidth}{0.02\textwidth} &Trees &3064   \\
					\crule[paviaU5]{0.02\textwidth}{0.02\textwidth} &Painted metal sheets &1345   \\
					\crule[paviaU6]{0.02\textwidth}{0.02\textwidth} &Bare Soil &5029   \\
					\crule[paviaU7]{0.02\textwidth}{0.02\textwidth} &Bitumen &1330   \\
					\crule[paviaU8]{0.02\textwidth}{0.02\textwidth} &Self-Blocking Bricks &3682   \\
					\crule[paviaU9]{0.02\textwidth}{0.02\textwidth} &Shadows &947    \\
				\end{tabular}&
				\begin{tabular}[t]{ccc}
					
					\textit{color}&\textit{land-cover type}&\textit{number}\\
					\hline
					\crule[salinas_corrected1]{0.02\textwidth}{0.02\textwidth} &Brocoli green weeds 1 &2009  \\
					\crule[salinas_corrected2]{0.02\textwidth}{0.02\textwidth} &Brocoli green weeds 2 &3726  \\
					\crule[salinas_corrected3]{0.02\textwidth}{0.02\textwidth} &Fallow &1976  \\
					\crule[salinas_corrected4]{0.02\textwidth}{0.02\textwidth} &Fallow rough plow &1394  \\
					\crule[salinas_corrected5]{0.02\textwidth}{0.02\textwidth} &Fallow smooth &2678  \\
					\crule[salinas_corrected6]{0.02\textwidth}{0.02\textwidth} &Stubble &3959  \\
					\crule[salinas_corrected7]{0.02\textwidth}{0.02\textwidth} &Celery &3579  \\
					\crule[salinas_corrected8]{0.02\textwidth}{0.02\textwidth} &Grapes untrained &11271  \\
					\crule[salinas_corrected9]{0.02\textwidth}{0.02\textwidth} &Soil vinyard develop &6203  \\
					\crule[salinas_corrected10]{0.02\textwidth}{0.02\textwidth} &Corn senesced green weeds &3278  \\
					\crule[salinas_corrected11]{0.02\textwidth}{0.02\textwidth} &Lettuce romaine 4wk &1068  \\
					\crule[salinas_corrected12]{0.02\textwidth}{0.02\textwidth} &Lettuce romaine 5wk &1927  \\
					\crule[salinas_corrected13]{0.02\textwidth}{0.02\textwidth} &Lettuce romaine 6wk &916  \\
					\crule[salinas_corrected14]{0.02\textwidth}{0.02\textwidth} &Lettuce romaine 7wk &1070  \\
					\crule[salinas_corrected15]{0.02\textwidth}{0.02\textwidth} &Vinyard untrained &7268  \\
					\crule[salinas_corrected16]{0.02\textwidth}{0.02\textwidth} &Vinyard vertical trellis &1807  \\
				\end{tabular}&
				\begin{tabular}[t]{ccc}
					
					\textit{color}&\textit{land-cover type}&\textit{number}\\
					\hline
					\crule[salinasA_corrected1]{0.02\textwidth}{0.02\textwidth} &Brocoli green weeds 1 &391  \\
					\crule[salinasA_corrected2]{0.02\textwidth}{0.02\textwidth} &Corn senesced green weeds &1343  \\
					\crule[salinasA_corrected3]{0.02\textwidth}{0.02\textwidth} &Lettuce romaine 4wk &616  \\
					\crule[salinasA_corrected4]{0.02\textwidth}{0.02\textwidth} &Lettuce romaine 5wk &1525  \\
					\crule[salinasA_corrected5]{0.02\textwidth}{0.02\textwidth} &Lettuce romaine 6wk &674  \\
					\crule[salinasA_corrected6]{0.02\textwidth}{0.02\textwidth} &Lettuce romaine 7wk &799  \\
				\end{tabular}\\
				\hline
				
			\end{tabular}}
		\caption{Number of samples in two Pavia scene dataset.}
		\label{psgt}
	\end{table}

	\subsection{Environment}\label{subsec:environment}
	We trained and tested the model on a PC with 4 NVIDIA RTX 2080Ti(11GB) GPUs, dual Xeon 4210 Processor
	and 128GB of memory.
	The operating system is Ubuntu server 18.04 x64 LTS with CUDA 10.1 and CUDNN 7.6.5.
	We implement the model with Tensorflow 1.14.0 and the model is trained and tested on GPU\@.

	\subsection{Experiment Setup}Usually, a HSI dataset has labeled and unlabeled pixels.
	In our experiment we discarded those unlabeled pixels, only using the labeled pixels for training and testing.
	We divided the labeled pixels into 3 groups in our experiment: training data, testing data and validating data.
	We randomly selected 10\% of the total labeled pixels as training set, then we randomly selected 100 labeled pixels
	and all pixels except those chosen for training were used to validate the model.
	The training and validating dataset are shown in figure~\ref{trainMaps} and figure~\ref{testMaps}

	\begin{figure}[htbp]
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=0.9\textwidth]{pic/p5/pc/1/img/trainMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=0.9\textwidth]{pic/p5/pu/1/img/trainMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=0.9\textwidth]{pic/p5/sl/1/img/trainMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=0.9\textwidth]{pic/p5/sa/1/img/trainMap.eps}
			\caption{}
		\end{subfigure}
		\caption{Training set for (a) Pavia Center, (b) Pavia University, (c) Salinas Valley and (d) Salinas A}
		\label{trainMaps}
	\end{figure}

	\begin{figure}[htbp]
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=0.9\textwidth]{pic/p5/pc/1/img/testMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=0.9\textwidth]{pic/p5/pu/1/img/testMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=0.9\textwidth]{pic/p5/sl/1/img/testMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=0.9\textwidth]{pic/p5/sa/1/img/testMap.eps}
			\caption{}
		\end{subfigure}
		\caption{Validating set for (a) Pavia Center, (b) Pavia University, (c) Salinas Valley and (d) Salinas A.}
		\label{testMaps}
	\end{figure}

	\subsection{Result and Analysis}\label{subsec:result-and-analysis}
	First of all, we listed the quantitative classification assessment using the PU, PC, SV and SA dataset considering
	SVM,
	1D CNN\cite{hu2015deep},
	2D CNN\cite{sharma2016hyperspectral},
	3D CNN\cite{hamida20183},
	3D FCN\cite{lee2016contextual},
	CNN-MRF\cite{cao2018hyperspectral},
	CapsNet and DCCN together in table~\ref{result}.
	From the results reported in table~\ref{result}, we can observe that our proposed DCCN model outperforms all the
	other seven state-of-art models on all 4 dataset.
	Among all competitors in this experiment, 3D CNN\cite{hamida20183} and the CapsNet model performed relatively second
	best, since both of the two model achieved second high accuracy on different dataset.
	This is expected because both of the two algorithm are spectral-spatial based so they can utilize more information,
	compared to the 2D CNN, FCN, SVM and CNN-MRF classifiers, to classify the HSI pixels.
	Nevertheless, our DCCN model can consistently outperform the second-best algorithm with an average \textbf{fill data
	here} for AA, OA, $\kappa$.

	\begin{table}[htb]
		\tiny
		\centering
		\begin{tabular}{|c|c|c|c|c|}
			\hline
			 &\textbf{PAVIA CENTER} & \textbf{PAVIA UNIVERSITY}&\textbf{SALINAS VALLEY} & \textbf{SALINAS A}\\
			\hline
			\begin{tabular}{c}
				\textbf{model}\\
				\hline
				{SVM}\\
				{1D CNN\cite{hu2015deep}}\\
				{2D CNN\cite{sharma2016hyperspectral}}\\
				{3D CNN\cite{hamida20183}}\\
				{3D FCN\cite{lee2016contextual}}\\
				{CNN-MRF\cite{cao2018hyperspectral}}\\
				{CapsNet}\\
				{DCCN}\\
			\end{tabular}&
			\begin{tabular}{ccc}
				\textbf{OA}&\textbf{AA}&\textbf{$\kappa$}\\
				\hline
				100\%&100\%&100\%\\
				100\%&100\%&100\%\\
				100\%&100\%&100\%\\
				100\%&100\%&100\%\\
				100\%&100\%&100\%\\
				100\%&100\%&100\%\\
				100\%&100\%&100\%\\
				100\%&100\%&100\%\\
			\end{tabular}&
			\begin{tabular}{ccc}
				\textbf{OA}&\textbf{AA}&\textbf{$\kappa$}\\
				\hline
				100\%&100\%&100\%\\
				100\%&100\%&100\%\\
				100\%&100\%&100\%\\
				100\%&100\%&100\%\\
				100\%&100\%&100\%\\
				100\%&100\%&100\%\\
				100\%&100\%&100\%\\
				100\%&100\%&100\%\\
			\end{tabular}&
			\begin{tabular}{ccc}
				\textbf{OA}&\textbf{AA}&\textbf{$\kappa$}\\
				\hline
				100\%&100\%&100\%\\
				100\%&100\%&100\%\\
				100\%&100\%&100\%\\
				100\%&100\%&100\%\\
				100\%&100\%&100\%\\
				100\%&100\%&100\%\\
				100\%&100\%&100\%\\
				100\%&100\%&100\%\\
			\end{tabular}&
			\begin{tabular}{ccc}
				\textbf{OA}&\textbf{AA}&\textbf{$\kappa$}\\
				\hline
				100\%&100\%&100\%\\
				100\%&100\%&100\%\\
				100\%&100\%&100\%\\
				100\%&100\%&100\%\\
				100\%&100\%&100\%\\
				100\%&100\%&100\%\\
				100\%&100\%&100\%\\
				100\%&100\%&100\%\\
			\end{tabular}\\
			\hline
		\end{tabular}
		\caption{Final result of all adopted models.}
		\label{result}
	\end{table}

	In figure~\ref{pupm,pcpm,svpm,sapm} we presented the classification maps outputted by our DCCN and seven competitor
	model after the training process.
	\begin{figure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/predictMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/predictMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/predictMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/predictMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/predictMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/predictMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/predictMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/predictMap.eps}
			\caption{}
		\end{subfigure}
		\label{pcpm}
	\end{figure}
	\begin{figure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/predictMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/predictMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/predictMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/predictMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/predictMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/predictMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/predictMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/predictMap.eps}
			\caption{}
		\end{subfigure}
		\label{pupm}
	\end{figure}
	\begin{figure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/predictMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/predictMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/predictMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/predictMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/predictMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/predictMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/predictMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/predictMap.eps}
			\caption{}
		\end{subfigure}
		\label{svpm}
	\end{figure}
	\begin{figure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/predictMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/predictMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/predictMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/predictMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/predictMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/predictMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/predictMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/predictMap.eps}
			\caption{}
		\end{subfigure}
		\label{sapm}
	\end{figure}

	Finally, we presented the probability maps the eight algorithms produced of the four HSI dataset in
	figure~\ref{pcpd ,pupd,svpd,sapd}.
	\begin{figure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/probMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/probMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/probMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/probMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/probMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/probMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/probMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/probMap.eps}
			\caption{}
		\end{subfigure}
		\label{pcpd}
	\end{figure}
	\begin{figure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/probMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/probMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/probMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/probMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/probMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/probMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/probMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/probMap.eps}
			\caption{}
		\end{subfigure}
		\label{pupd}
	\end{figure}
	\begin{figure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/probMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/probMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/probMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/probMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/probMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/probMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/probMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/probMap.eps}
			\caption{}
		\end{subfigure}
		\label{svpd}
	\end{figure}
	\begin{figure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/probMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/probMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/probMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/probMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/probMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/probMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/probMap.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.125\textwidth}
			\includegraphics[width=0.95\textwidth]{pic/p5/pc/1/img/probMap.eps}
			\caption{}
		\end{subfigure}
		\label{sapd}
	\end{figure}

	\subsection{Convergence analysis}\label{subsec:convergence}
	In this subsection, we evaluated the convergence of our proposed network.
	Figure~\ref{accandloss} showed the training process of our DCCN model for 4 dataset separately.
	\begin{figure}[htb]
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=\textwidth]{pic/p5/pc/1/img/lossAndAcc.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=\textwidth]{pic/p5/pu/1/img/lossAndAcc.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=\textwidth]{pic/p5/sl/1/img/lossAndAcc.eps}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=\textwidth]{pic/p5/sa/1/img/lossAndAcc.eps}
			\caption{}
		\end{subfigure}
		\caption{Accuracy and loss of the proposed DCCN model with increasing iterations during the training process.
		The horizontal axis represents 100 training epochs.
		The corresponding relationship between the plot and dataset are:(a) for Pavia Center, (b) for Pavia University,
			(c) for Salinas Vallet and (d) for Salinas A.}
		\label{accandloss}
	\end{figure}


	\section{DISCUSSION}\label{sec:discussion}
	In this sector, we further explored the effect of different hyper-parameters to the DCCN\@.
	We used the Pavia University and Salinas A dataset to train and test our model, so that we can observe the model's
	performance on both large and small dataset.
	We first tested the effect of different size of training set with a fixed patch size of 7*7, and then tested the
	effect of different patch sizes.


	\section{CONCLUSION}\label{sec:conclusion}


	\bibliography{cite}
\end{document}
