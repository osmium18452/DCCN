\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{fullpage}
\usepackage{graphicx}

\title{Spectral-Spatial Hyper-spectral Image Classification Using Dual-Channel Capsule Networks}
\author{liu wenbo}
\date{March 2020}


\begin{document}
	\bibliographystyle{IEEEtran}

	\maketitle


	\section{ABSTRACT}\label{sec:abstract}
	Deep learning methods have shown their marvel performance on hyper-spectral image (HSI) classification tasks.
	In particular,algorithms based on Convolution Neural Network (CNN) outperforms most of the conventional machine-
	learning based algorithms.
	Recently, a newly proposed neural network called Capsule Network (CapsNet) showed its potential to replace the CNNs
	in various classification tasks with its amazing performance.
	In this paper, we proposed a new network architecture based on CapsNet for HSI classification tasks, called
	Dual-channel Capsule Network (DCCN).
	Our DCCN model extracts features from spectral and spatial domain respectively with two separate convolution
	channels and then concatenates and feds the features into the capsule layer to classify each of the HSI pixels.
	The model is trained and tested on \textbf{(2/3/4/5?)} real HSI dataset \textbf{(Pavia University, Pavia City,
	Salinas Valley, Salinas A and KSC)} and achieved high accuracy.
	We also compared our network with some of the state-of-art models and found our model outperformed these models.
	Among these state-of-art models we also implemented the Capsule Network that has the same architecture as our
	network except the spectral convolution channel, and we found the DCCN model was superior to it as well.

	\noindent \textbf{Key Words:} Dual-Channel Capsule Network, Hyper-Spectral Image, Image Classification.


	\section{INTRODUCTION}\label{sec:introduction}
	With technology developing, high resolution hyper-spectral images (HSI) became readily available.
	Containing hundreds of spectral bands, HSI becomes a ideal method to further explore the earth's
	surface\cite{du2013foreword,bioucas2013hyperspectral,shippert2003introduction}.
	With a wealth of information, HSI has been applied in various fields, e.g.,
	land cover classification\cite{yan2015urban},
	object detection\cite{eslami2015developing},
	forest inventory\cite{matsuki2015hyperspectral}
	and water source management\cite{govender2007review}.
	Therefore, HSI classification has become a quite significant problem to solve.
	It aims at assigning a specific label to each pixel according to its spectral-spatial information\cite{wang2018scene}.

	The remainder of the paper is organized as follows.
	In section~\ref{sec:related-works}, we introduce some related work of our paper.
	In section~\ref{sec:proposed-method}, we describe the structure of our proposed network, how we pre-process the data
	and how to train the network.
	In section~\ref{sec:experiment}, we present and analyze the result of out experiment.
	In section~\ref{sec:discussion}, we further explore the potential of DCCN\@.
	Finally, in section~\ref{sec:conclusion}, we conclude this paper with some remarks and hints at plausible future
	research lines.


	\section{RELATED WORK}\label{sec:related-works}

	\subsection{Capsule Network}\label{subsec:capsule-network}

	\subsection{Margin Loss}\label{subsec:margin-loss}


	\section{PROPOSED METHOD}\label{sec:proposed-method}

	In this section, we introduce our proposed algorithm.
	We first described how we pre-process the HSI data, then we \textbf{depicted} the network structure and finally we
	introduced how to train the network.

	\subsection{Data Pre-Process}\label{subsec:data-pre-process}
	The original HSI dataset is usually a chunk of 3D tensor that is too large to feed into the neural network, so we
	have to do some process before input it into the DCCN\@.
	On one hand, we extracted the pixel with its neighbour pixels' spectrum and grouped them as a spectral cube, namely
	a patch.
	On the other, we extracted the single pixel's spectrum as a 1D vector.
	The pixel's spectrum vector will be combined with its corresponding patch to feed into the DCCN model simultaneously.
	The patch an be assigned with different size.
	In our experiment, we set it as 7×7×$n_{bands}$.

	Usually, a HSI dataset has labeled and unlabeled pixels.
	In our experiment we discarded those unlabeled pixels, only using the labeled pixels for training and testing.
	We divided the labeled pixels into 3 groups in our experiment: training data, testing data and validating data.
	We randomly selected 10\% of the total labeled pixels as training set, then we randomly selected 100 labeled pixels
	and the rest of the labeled pixels was used to validate the model.

	\subsection{Network Architecture}\label{subsec:network-architecture}
	\begin{figure}[!ht]
		\centering
		\includegraphics[width=0.4\textwidth]{pic/lossAndAcc.eps}
		\caption{Proposed network architecture.}
		\label{netArct}
	\end{figure}
	Our proposed network architecture is showed in~\ref{netArct}.
	As is shown, our Dual-Channel Capsule Network contains two convolution channel, i.e., a 2D convolution channel to
	process the patch and a 1D convolution channel to process the spectrum, and a capsule channel.
	As we can see, the Dual-Channel Capsule Network we adopted is a very shallow network, containing only six layers,
	that is, a convolution layer append with a activation layer and a primary capsule layer in both of the spectral
	channel and the spatial channel, two capsule layer in the fully connected capsule channel and a output layer to
	process the output from the latter capsule layer.

	To be specific, the detailed parameter of the DCCN deployed in our experiment are as follows.
	For the spatial channel, the first layer is a 2D convolution layer, including 50 filters with the kernel size of
	3×3 and stride of 1.
	The following activation layer uses ReLU function as activation function.
	The primary capsule layer uses 64 filters with kernel size of 3×3 and stride of 1.
	Its output capsules' dimension is 8.
	For the spectral channel, the first layer is a 1D convolution layer,including 30 filters with kernel size of 32 and
	stride of 8.
	The following activation layer used ReLU activation function as well.
	The primary capsule layer uses 64 filters with kernel size of 3 and stride of 1.
	Its output capsules' dimension is 8 so that it can be concatenated with the spatial channel's output capsules and
	then fed into the fully connected capsule layer.
	For the first capsule layer, it has \textbf{100 or so?} capsules, which each contains 8 neurons inside.
	The second capsule layer has $n_{classes}$ capsules with 16 neurons inside, where $n_{classes}$ denotes how many
	different classes of pixels inside the HSI dataset.
	So far, the main part of the DCCN model has finished.
	But as is mentioned before, the method we proposed is end-to-end, so we added a output layer to process the output
	of the network.
	The output layer will squash the 2D tensor ,made up by capsules, into a 1D vector (Let's call the 1D vector
	probability vector ,prob-vector in short, since each of its elements represents the probability of a pixel should
	be assigned to the according class.) by replacing each of the capsule with its module and perform argument-max
	($argmax$) function to gain the pixel's label.


	And here's how the data flows in the DCCN.
	First, the sliced patch is fed into the 2D convolution channel to extract features spatially, and the spectrum
	of the patch's center pixel is fed into the 1D convolution channel to extract features in spectral domain in
	parallel.
	Then, the extracted features, both spectral and spatial, are concatenated and fed into the capsule channel to output
	the vectors (that is, the capsule) whose module represents the probability of a pixel being assigned a specific
	label.
	And finally, the output layer will squash the outputted 2D tensor and assign a label to the pixel by performing
	$argmax$ function to the prob-vector.

	\subsection{Train the Network}\label{subsec:train-the-network}
	At first, we randomly shuffled the training set and divided it into several batches.
	These batches ,instead of single pixel, would be feed into the model for training to accelerate the training process.
	In our experiment, considering the training speed and final accuracy, we set the batch size as 50.


	\section{EXPERIMENT}\label{sec:experiment}

	\subsection{Dataset Introduction}\label{subsec:dataset-introduction}

	\subsection{Environment}\label{subsec:environment}

	\subsection{Result and Analysis}\label{subsec:result-and-analysis}


	\section{DISCUSSION}\label{sec:discussion}

	\subsection{Convergence analysis}\label{subsec:convergence}


	\section{CONCLUSION}\label{sec:conclusion}


	\bibliography{cite}
\end{document}
